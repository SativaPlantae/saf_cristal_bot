import os
import streamlit as st
import pandas as pd
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory

# ğŸŒ¿ Carrega variÃ¡veis de ambiente
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

# ğŸŒ± ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(page_title="Agente SAF Cristal ğŸŒ±", layout="wide")
st.title("ğŸ¦— Agente Inteligente do SÃ­tio Cristal")
st.markdown("Converse com o agente sobre os dados do SAF. Pode perguntar como se fosse pra um amigo â€” ele fala a sua lÃ­ngua!")

# ğŸ“Š Carrega a planilha
df = pd.read_csv("dados/data.csv")

# ğŸ” Inicializa histÃ³rico se ainda nÃ£o existir
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ğŸ‘‹ Mensagem de boas-vindas
if len(st.session_state.chat_history) == 0:
    with st.chat_message("assistant", avatar="ğŸ¦—"):
        st.markdown("""
OlÃ¡! ğŸ˜Š  
Eu sou o **SAFBot**, seu ajudante aqui no SÃ­tio Cristal ğŸŒ½  
Pode me perguntar qualquer coisa sobre o projeto agroflorestal â€” desde o que estÃ¡ sendo produzido atÃ© como tudo funciona.  
Prometo responder sem enrolaÃ§Ã£o, de um jeito fÃ¡cil de entender. Vamos nessa? ğŸŒ±
""")

# ğŸ§  Prompt natural e amigÃ¡vel com memÃ³ria
prompt_template = PromptTemplate.from_template("""
VocÃª Ã© o SAFBot, um assistente simpÃ¡tico que responde com carinho, sem termos tÃ©cnicos.
Explique de forma simples, como se estivesse conversando com alguÃ©m da zona rural que nunca ouviu falar de SAF ou IA.
Use o contexto da conversa para continuar respondendo de forma natural e acolhedora.

HistÃ³rico:
{chat_history}

Pergunta: {pergunta}
""")

# ğŸ¤– Inicializa modelo e cadeia com memÃ³ria
llm = ChatOpenAI(
    temperature=0.2,
    model="gpt-4o",
    openai_api_key=openai_key
)

chain = LLMChain(
    llm=llm,
    prompt=prompt_template,
    memory=st.session_state.memory,
    verbose=False
)

# Entrada do usuÃ¡rio
query = st.chat_input("O que vocÃª quer saber sobre o SAF?")

# Exibe o histÃ³rico anterior
for user_msg, bot_msg in st.session_state.chat_history:
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸŒ¾"):
        st.markdown(user_msg)
    with st.chat_message("assistant", avatar="ğŸ¦—"):
        st.markdown(bot_msg)

# Se houver nova pergunta
if query:
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸŒ¾"):
        st.markdown(query)

    with st.chat_message("assistant", avatar="ğŸ¦—"):
        with st.spinner("Consultando os dados e lembranÃ§as do SAFBot..."):
            try:
                resposta = chain.run(pergunta=query)
            except Exception as e:
                resposta = f"âŒ Ocorreu um erro: {e}"
        st.markdown(resposta)

    # Atualiza o histÃ³rico visÃ­vel
    st.session_state.chat_history.append((query, resposta))

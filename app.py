import os
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAI
from langchain_experimental.agents import create_pandas_dataframe_agent
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# ğŸŒ¿ Carrega variÃ¡veis de ambiente
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

# ğŸŒ± ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(page_title="Agente SAF Cristal ğŸŒ±", layout="wide")
st.title("ğŸ Agente Inteligente do SÃ­tio Cristal")
st.markdown("Converse com o agente sobre os dados do SAF. Ele fala fÃ¡cil, como quem troca ideia na varanda!")

# ğŸ“Š Carrega a planilha
df = pd.read_csv("dados/data.csv", sep=";")

# ğŸ§  MemÃ³ria de conversa
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="history", return_messages=True)

# ğŸ§¾ HistÃ³rico visÃ­vel
if "visible_history" not in st.session_state:
    st.session_state.visible_history = []
    with st.chat_message("assistant", avatar="ğŸ"):
        st.markdown("""
OlÃ¡! ğŸ˜Š  
Eu sou o **SAFBot**, um ajudante do **SÃ­tio Cristal**. Estou aqui pra bater um papo gostoso com vocÃª e explicar tudo sobre nosso sistema agroflorestal. ğŸŒ±ğŸ’¬  
Quer saber quais espÃ©cies temos? Quanto rendeu um certo ano? Ou o que Ã© exatamente um SAF? Pode perguntar sem medo! Eu explico tudo de um jeito bem simples e direto, como se estivÃ©ssemos conversando na varanda. ğŸğŸ’›

---
ğŸ“Œ Exemplos do que vocÃª pode perguntar:
- Quais espÃ©cies tem no SAF Cristal?
- Qual foi o lucro em 2040?
- O que Ã© um SAF?
- Como esse sistema ajuda o meio ambiente?
        """)

for user_msg, bot_msg in st.session_state.visible_history:
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸŒ¾"):
        st.markdown(user_msg)
    with st.chat_message("assistant", avatar="ğŸ"):
        st.markdown(bot_msg)

# ğŸ¤– Modelos
llm_chat = ChatOpenAI(temperature=0.3, model="gpt-4o", openai_api_key=openai_key)
llm_agent = OpenAI(temperature=0.3, openai_api_key=openai_key)

# ğŸ“Š Agente com acesso ao DataFrame
agent = create_pandas_dataframe_agent(
    llm=llm_agent,
    df=df,
    verbose=False,
    handle_parsing_errors=True,
    allow_dangerous_code=True
)

# FunÃ§Ãµes auxiliares

def faturamento_total(df):
    return df["faturamento (R$)"].sum()

def lucro_total(df):
    return df["lucro (R$)"].sum()

def despesas_total(df):
    return df["despesas (R$)"].sum()

def anos_de_duracao(df):
    return len(df["anos"].unique())

def media_anual(df, coluna):
    return df.groupby("anos")[coluna].sum().mean()

def media_mensal(df, coluna):
    return media_anual(df, coluna) / 12

def maior_menor_faturamento(df):
    faturamento_ano = df.groupby("anos")["faturamento (R$)"].sum()
    maior = faturamento_ano.idxmax()
    menor = faturamento_ano.idxmin()
    return maior, menor

# ğŸ” Detecta se deve consultar a planilha

def pergunta_envia_para_planilha(texto):
    palavras_chave = [
        "lucro", "renda", "espÃ©cies", "produzindo", "produÃ§Ã£o", "anos", "quantos",
        "qual foi", "em", "faturamento", "quanto gerou", "valores", "total"
    ]
    return any(p in texto.lower() for p in palavras_chave)

# Entrada do usuÃ¡rio
query = st.chat_input("Pode perguntar qualquer coisa sobre o SAF Cristal!")

if query:
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸŒ¾"):
        st.markdown(query)

    if pergunta_envia_para_planilha(query):
        with st.spinner("Consultando os dados do SÃ­tio Cristal... ğŸ“Š"):
            try:
                resposta_dados = agent.run(query)
            except Exception as e:
                resposta_dados = f"[Ops! NÃ£o consegui pegar os dados certos agora: {str(e)}]"
    else:
        resposta_dados = ""

    input_completo = (
        "VocÃª Ã© o SAFBot ğŸ, um ajudante do SÃ­tio Cristal. "
        "Explique tudo com jeitinho simples, sem termos tÃ©cnicos, como se estivesse conversando com alguÃ©m da zona rural. "
        "Fale de forma acolhedora e use linguagem fÃ¡cil. Responda com base nisso, e nos dados abaixo, se houver:\n\n"
        f"Pergunta: {query}\n"
        f"{resposta_dados}"
    )

    resposta = llm_chat.invoke(input_completo)

    with st.chat_message("assistant", avatar="ğŸ"):
        st.markdown(resposta)

    st.session_state.visible_history.append((query, resposta))

